---
title: "Eat Out To Help Out"
author: "Barry Bullas"
date: "31/07/2020"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE, warning =FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Check if the required packages are installed, if not install them
list_of_packages <- c("dplyr", "stringr", "tidyverse", "sf", "DT")
new_packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

#Load required packages
lapply(list_of_packages, library, character.only = TRUE)
```

## Retaurant Map
I chose ESRI ArcGIS to produce the map, using a free ARC GIS Online account and hosting the data on GitHub.  You can view the finished [Eat Out To Help Out Restaurant Map](https://arcg.is/15aOjm0) on ESRI's ARC GIS Online website.  This project also contains a map in leaflet which covers any restaurant with an NE postcode.

> This project uses the data provided by HMRC but this project and the resulting map is a personal project and not an offical HMRC product.

## Aim of the Project

This aim of this project is to produce a map displaying the establishments that are taking part in HMRC's Eat Out To Help Out (EO2HO) scheme.  
HMRC provides a [Restaurant Finder](https://www.tax.service.gov.uk/eat-out-to-help-out/find-a-restaurant/?_ga=2.159416701.1723075929.1596201136-857574019.1589147008), however it only returns a list of restaurants within a set distance.  I wanted to produce a map that would allow users to explore restauraunts in their own area.

## The Scheme
The scheme operates From 3 to 31 August.  Customers get a 50% discount, up to a maximum of Â£10 per person, when you eat in at restaurants that are registered with the Eat Out to Help Out Scheme.

More details on the scheme can be found on the [Gov.UK site](https://www.gov.uk/guidance/get-a-discount-with-the-eat-out-to-help-out-scheme).

The [EO2HO Registration data is provided by HMRC](https://www.gov.uk/government/publications/eat-out-to-help-out-scheme-registration-data-for-software-developers) and can be found on thee [HMRC Github Account](https://github.com/hmrc/eat-out-to-help-out-establishments). The data I used was downloaded on 31st July 2020. 


## Office of National Statistics Postcode Directory (ONSPD)


The HMRC data only provides the address information.  To get the coordinates need for the map I use the Office of National Statistics Postcode Directory (ONSPD) provide free by the Office of National Statistics (ONS).  The ONSPD allows you to identify a lattitude and Longitude for the postcodes in the EA2HO data. I initially used the National Statistics Postcode Lookup (NSPL) But couldn't match around 70 restauratn postcodes to coordinates.  The ONSPD contains current and terminated postcodes so should provide a better match.  **update: I could't find the missing postodes in the ONSPD either** 

The ONSPD is a great tool for UK statistical geographies.  It can be downloaded freely from the [ONS Open Geography Portal](https://geoportal.statistics.gov.uk/datasets/ons-postcode-directory-february-2020).  I used the Feb 2020 version which was the latest available.  I downloaded it to the data folder.  The file is too big to upload to Github so you will need to download it yourself.

```{r subset_onspd}
#Load the onspd dataset which has coordinates for postcodes
onspd <-read.csv('data/ONSPD_FEB_2020_UK.csv')

#Keep only the columns we need
onspd <- subset(onspd, select = c(pcds,lat, long))

#Rename pcds colum to make join to EO2HO easier
names(onspd) <- c("Postcode", "lat", "lng")

#write to its own csv in case needed again
write.csv(onspd, 'data/onspd_short.csv', row.names = FALSE)
```

## Ordnance Survey - Codepoint
Because a number of postcodes provide can't be matched to the NSPL, i tried using the  [Ordnace Survey Codepoint dataset](https://osdatahub.os.uk/downloads/open) to see if it matched any better.  Codepoint Open provides coordinates for all GB postcodes (Northern Ireland is not included which is a significant limitation) and is available to download for free.  OS provides a range of documents to [get you started](https://www.ordnancesurvey.co.uk/business-government/tools-support/code-point-open-support).

Codepoint provides a csv file for each Area code (the first one or two characters).  I combined these quickly and simply using the command prompt.  To do this open a command prompt (click the start menu and type command).  Change the current directory using the CD command, to the folder containing all the csv files.

> CD c:\folder_with_csvs\

Then use the copy command to merge all the csv files into one csv called codepoint.

> copy *.csv codepoint.csv



```{r}
codepoint <- read.csv('data/codepoint_20200820.csv', header=FALSE, stringsAsFactors = FALSE)

#Add column names
names(codepoint) <- c('Postcode','Positional_quality_indicator','Eastings', 'Northings', 'Country_code', 'NHS_regional_HA_code', 'NHS_HA_code', 'Admin_county_code', 'Admin_district_code', 'Admin_ward_code')

#Drop uneeded columns
codepoint <- codepoint[c('Postcode', 'Eastings', 'Northings')]

#Convert Northing/Easting to Latitude and Longitude
codepoint_coordinates <- codepoint %>%
  st_as_sf(coords = c("Eastings", "Northings"), crs = 27700) %>%
  st_transform(4326) %>%
  st_coordinates() %>%
  as_tibble()


#bind coordinates to original data
codepoint <- cbind(codepoint, codepoint_coordinates)
rm(codepoint_coordinates)

#Rename columns
codepoint <- rename(codepoint, c("lng"="X", "lat"="Y"))

#Format the postcodes to match the other datasets
codepoint$Postcode<- toupper(str_replace_all(codepoint$Postcode, " ", ""))

codepoint$Postcode<-paste(substr(codepoint$Postcode, 1,nchar(codepoint$Postcode)-3), substr(codepoint$Postcode,nchar(codepoint$Postcode)-2,nchar(codepoint$Postcode)), sep = " ", collapse = NULL)

#Drop uneeded columns
codepoint <- codepoint[c('Postcode', 'lng', 'lat')]

write.csv(codepoint, 'data/codepoint_subset.csv')

```


## Restaurant Data

In this section I format the postcodes in the restaurant data to ensure they join with the onspd postcodes.  I then combine the address columns into a single column which can be used in a popup on the map.  

```{r load_data}
#load onspd data
onspd <- read.csv('data/onspd_short.csv')

#load codepoint
codepoint <- read.csv('data/codepoint_subset.csv')

#Load the Eat Out To Help Out (EO2HO) data downloaded from GitHub
#eo2ho <- read.csv('data/participating-establishments/restaurants.csv')

#Load directly from github
eo2ho <- read.csv('https://raw.githubusercontent.com/hmrc/eat-out-to-help-out-establishments/master/data/participating-establishments/restaurants.csv')

#Format the postcodes to match the onspd data
eo2ho$Postcode<- toupper(str_replace_all(eo2ho$Postcode, " ", ""))

eo2ho$Postcode<-paste(substr(eo2ho$Postcode, 1,nchar(eo2ho$Postcode)-3), substr(eo2ho$Postcode,nchar(eo2ho$Postcode)-2,nchar(eo2ho$Postcode)), sep = " ", collapse = NULL)

#Join EO2HO data to onspd to get a cordinate for each establishment
eo2ho_coordinates <- merge(eo2ho, onspd, by="Postcode", all = TRUE)

#Join EO2HO data to Codepoint to get a cordinate for each establishment
#eo2ho_coordinates <- merge(eo2ho, codepoint, by="Postcode", all = TRUE)


# Keep only restaurants 
eo2ho_coordinates <- subset(eo2ho_coordinates, !is.na(eo2ho_coordinates$Name))

#Subset any that didn't match to the lookup, for investigation
no_coordinates<- subset(eo2ho_coordinates, is.na(eo2ho_coordinates$lat))

#Remove those that we couldn't get coordinates for, from the final dataset
eo2ho_coordinates <- subset(eo2ho_coordinates, !is.na(eo2ho_coordinates$lat))

#Add address column for popu
eo2ho_coordinates$Address <- paste(eo2ho_coordinates$Name, eo2ho_coordinates$Line.1, eo2ho_coordinates$Line.2, eo2ho_coordinates$Town, eo2ho_coordinates$County, eo2ho_coordinates$Postcode, sep=", ")

#A smaller subset that can be used for testing the leaflet map
eo2ho_coordinates_NE<- eo2ho_coordinates %>% filter(str_detect(Postcode,"^NE"))


#Drop uneeded columns
eo2ho_coordinates <- subset(eo2ho_coordinates, select = c(lat, lng, Address))
eo2ho_coordinates_NE <- subset(eo2ho_coordinates_NE, select = c(lat, lng, Address))

#remove the original datasets to free up memory
rm(onspd, eo2ho, codepoint)

#Write the data to a new csv
write.csv(eo2ho_coordinates,'data/eo2ho_coordinates.csv', row.names = FALSE)
write.csv(eo2ho_coordinates_NE,'data/eo2ho_coordinates_NE.csv', row.names = FALSE)
write.csv(no_coordinates,'data/no_coordinates.csv', row.names = FALSE)
```

### Investigate issues

Comparing the match rates for the different postcode lookups, I have chosen to stick with the ONSPD.  Not having Northern Irish postcodes is a significant limitation.  In addition, there are still more unmatched postcodes when the Northern Irish postcodes are excluded.

```{r}
#extract the area from the postcode
no_coordinates$Area <- str_extract(no_coordinates$Postcode, "^[:alpha:]{1,2}")


area_breakdown <- no_coordinates %>%
  group_by(Area) %>%
  tally(sort = TRUE, name="unmatched_count")


datatable(area_breakdown,
          caption = 'Unmatched postcodes by area',
          class = 'cell-border stripe',
          rownames = FALSE,
          options = list(searching = FALSE)
)

#Codepoint does not provide data for Northern Ireland so we have 1,878 unmatched for BT and 423 for elsewhere
#ONSPD has a better match rate with 98 unmatched
```

